{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "#url = 'https://www.tekstowo.pl/artysci_na,A,strona,1.html'\n",
    "pages = list(range(1, 11))\n",
    "\n",
    "domain = 'https://tekstowo.pl'\n",
    "urls = []\n",
    "cnt = 0\n",
    "\n",
    "# Extracting the artist list\n",
    "for page in pages:\n",
    "\turl = f'https://www.tekstowo.pl/artysci_na,A,strona,{page}.html'\n",
    "\ttry:\n",
    "\t\treq = requests.get(url)\n",
    "\t\tif req.ok:\n",
    "\t\t\tsoup = BeautifulSoup(req.content, 'lxml')\n",
    "\t\t\tfor link in soup.find_all('a'):\n",
    "\t\t\t\titem = link.get('href')\n",
    "\t\t\t\t#if type(link.get('href')) == str and 'piosenki_' in link.get('href'):\n",
    "\t\t\t\tif isinstance(item, str) and 'piosenki_' in item:\n",
    "\t\t\t\t\turls.append(link.get('href'))\n",
    "\n",
    "\t\t\t#print(soup.prettify())\n",
    "\t\t\t#art = soup.find_all(class_='title')\n",
    "\n",
    "\t\t\t#print(art)\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tcnt += 1\n",
    "\t\tprint(e)\n",
    "\n",
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages_number(url: str) -> int:\n",
    "\t\"\"\"\n",
    "\tGet the highest page number per given letter.\n",
    "\t\"\"\"\n",
    "\tmax_page = 0\n",
    "\n",
    "\treq = requests.get(url)\n",
    "\tif req.ok:\n",
    "\t\tsoup = BeautifulSoup(req.content, 'lxml')\n",
    "\t\tfor page in soup.find_all(class_='page-link'):\n",
    "\t\t\tif page.text.isnumeric() and int(page.text) > max_page:\n",
    "\t\t\t\tmax_page = int(page.text)\n",
    "\t#print(f\"Max page: {max_page}\")\n",
    "\treturn max_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import time\n",
    "\n",
    "def create_lut_pagination():\n",
    "\t\"\"\"\n",
    "\tCreates a look-up table for each letter containing max page number\n",
    "\t\"\"\"\n",
    "\n",
    "\talphabet = list(string.ascii_uppercase) + ['pozostale']\n",
    "\n",
    "\tlut_pages = {}\n",
    "\n",
    "\tfor letter in alphabet:\n",
    "\t\t\ttime.sleep(5)\n",
    "\t\t\turl = f\"https://www.tekstowo.pl/artysci_na,{letter}.html\"\n",
    "\t\t\tlut_pages[letter] = get_pages_number(url)\n",
    "\t\t\tprint(f\"Letter {letter} has {lut_pages[letter]} pages of artists.\")\t\n",
    "\t\n",
    "\treturn lut_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_artists(letter, max_page_per_letter):\n",
    "\t\"\"\"\n",
    "\tScrape all of the artists starting with a given letter in the alphabet.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\turls = []\n",
    "\tlimit = max_page_per_letter[letter]+1\n",
    "\n",
    "\tfor page in range(1, limit):\n",
    "\n",
    "\t\turl = f\"https://www.tekstowo.pl/artysci_na,{letter},strona,{page}.html\"\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\ttime.sleep(5)\n",
    "\t\t\tresponse = requests.get(url)\n",
    "\t\t\tif response.ok:\n",
    "\t\t\t\tsoup = BeautifulSoup(response.content, 'lxml')\n",
    "\t\t\t\tfor link in soup.find_all('a'):\n",
    "\t\t\t\t\titem = link.get('href')\n",
    "\t\t\t\t\tif type(item) == str and 'piosenki_' in item:\n",
    "\t\t\t\t\t\turls.append('https://tekstowo.pl' + item)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(e)\n",
    "\t\tprint(f\"{letter}: Visited {page}/{limit-1}\")\n",
    "\t\t\n",
    "\tprint(f\"Letter {letter}: collected {len(urls)} artists\")\n",
    "\treturn urls, len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def get_artist_songs(artist_url):\n",
    "    urls = []\n",
    "    processed_first_page = False\n",
    "\n",
    "    while not processed_first_page:\n",
    "        time.sleep(5)\n",
    "        try:\n",
    "            response = requests.get(artist_url)\n",
    "            if response.ok:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                songs = soup.find_all(class_='box-przeboje')\n",
    "                artist = soup.find(class_=\"col-md-7 col-lg-8 px-0\")\n",
    "                artist = artist.text.split(\" (\")[0].strip()\n",
    "                \n",
    "                for song in songs:\n",
    "                    song_title_element = song.find(class_='title')\n",
    "\n",
    "                    if song_title_element:\n",
    "                        if artist in song_title_element.text.strip():\n",
    "                            song_url = \"https://tekstowo.pl\" + song_title_element['href']\n",
    "                            if not \".plpiosenka\" in song_url and song_url not in urls:\n",
    "                                urls.append(song_url)\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                button_next_page = soup.find_all(class_='page-link')\n",
    "                if button_next_page and len(button_next_page) > 0:\n",
    "                    button_next_page = button_next_page[-1]\n",
    "\n",
    "                    if 'nastÄ™pna' in button_next_page.text.lower():\n",
    "                        artist_url = \"https://tekstowo.pl\" + button_next_page['href']\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "    print(artist)\n",
    "    print(f\"Collected {len(urls)} song URLs\")\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def extract_song(song_url):\n",
    "\tsong = \"\"\n",
    "\tsong_translation = None\n",
    "\n",
    "\ttry:\n",
    "\t\tresponse = requests.get(song_url)\n",
    "\t\tif response.ok:\n",
    "\t\t\tsoup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "\t\t\t# Song title\n",
    "\t\t\tsong_title = soup.find(class_='col-lg-7').text.strip()\n",
    "\t\t\n",
    "\t\t\t# Original song\n",
    "\t\t\tsong_html = soup.find(class_='inner-text')\t\t\t\n",
    "\t\t\tsong = song_html.text.strip()\n",
    "\n",
    "\t\t\t# Translated version\n",
    "\t\t\ttransl_html = soup.find('div', id='translation')\n",
    "\t\t\tsong_translation = transl_html.text.strip()\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tprint(e)\n",
    "\n",
    "\treturn song, song_translation, song_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langdetect\n",
    "\n",
    "def assess_language(song_text):\n",
    "\tif not isinstance(song_text, str):\n",
    "\t\treturn False\n",
    "\t\n",
    "\t#elif len(song_text) == 0:\n",
    "\t\t#raise TypeError(\"Empty file\")\n",
    "\n",
    "\telse:\n",
    "\t\ttry:\n",
    "\t\t\treturn langdetect.detect(song_text)\n",
    "\t\texcept langdetect.lang_detect_exception.LangDetectException:\n",
    "\t\t\treturn \"Language detection failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_songs(title, song_1, song_2, lang_1, lang_2, letter):\n",
    "\t\"\"\"\n",
    "\tSave the scraped song with corresponding languages\n",
    "\tand title.\n",
    "\t\"\"\"\n",
    "\n",
    "\tsave_dir = \"teksty/\"\n",
    "\n",
    "\tclean_title = title.replace('/', '-')\n",
    "\t\n",
    "\tsong_1_filename = f\"{letter}/{clean_title}__{lang_1.upper()}.txt\"\n",
    "\tsong_2_filename = f\"{letter}/{clean_title}__TRAN__{lang_2.upper()}.txt\"\n",
    "\n",
    "\twith open(os.path.join(save_dir, song_1_filename), 'w', encoding='utf-8') as f:\n",
    "\t\tf.write(song_1)\n",
    "\t\tf.close()\t\n",
    "\t\tprint(f\"Original successfully saved: {title}\")\n",
    "\n",
    "\tif len(lang_2) < 3 or song_2 == 0:\n",
    "\t\twith open(os.path.join(save_dir, song_2_filename), 'w', encoding='utf-8') as f:\n",
    "\t\t\tf.write(song_2)\n",
    "\t\t\tf.close\n",
    "\t\t\tprint(f\"Translation successfully saved: {title}\")\n",
    "\t\n",
    "\telse:\n",
    "\t\tprint(\"No translation found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'pozostale']\n"
     ]
    }
   ],
   "source": [
    "alphabet = list(string.ascii_lowercase.upper()) + [\"pozostale\"]\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_page_per_letter = create_lut_pagination()\n",
    "\n",
    "for letter in alphabet:\n",
    "    cnt = 0\n",
    "\n",
    "    artist_urls, artist_cnt = get_artists(letter, max_page_per_letter)\n",
    "    print(f\"Letter {letter} artists collected.\")\n",
    "\n",
    "    for artist_url in artist_urls:\n",
    "        artist_songs = get_artist_songs(artist_url)\n",
    "        #print(f\"Artist\")\n",
    "\n",
    "        try:\n",
    "            for artist_song in artist_songs:\n",
    "                text1, text2, title = extract_song(artist_song)\n",
    "                lang1 = assess_language(text1)\n",
    "                lang2 = assess_language(text2)\n",
    "\n",
    "                save_songs(title, text1, text2, lang1, lang2)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "        cnt += 1\n",
    "    \n",
    "    print(f\"Letter {letter}: processed {cnt}/{artist_cnt}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Visited 1/539\n",
      "A: Visited 2/539\n"
     ]
    }
   ],
   "source": [
    "#letter = \"Q\"\n",
    "\n",
    "for letter in alphabet:\n",
    "\t\n",
    "\tcnt = 0\n",
    "\n",
    "\tartist_urls, artist_cnt = get_artists(letter, max_page_per_letter)\n",
    "\tprint(f\"Letter {letter} artists collected.\")\n",
    "\n",
    "\n",
    "\t\t\n",
    "\tfor artist_url in artist_urls:\n",
    "\t\tprint(artist_url)\n",
    "\t\tartist_songs = get_artist_songs(artist_url)\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\ttime.sleep(5)\n",
    "\t\t\tfor artist_song in artist_songs:\n",
    "\t\t\t\ttext1, text2, title = extract_song(artist_song)\n",
    "\t\t\t\t\n",
    "\t\t\t\tif len(text1) > 10:\n",
    "\t\t\t\t\tlang1 = assess_language(text1)\n",
    "\t\t\t\tif len(text2) > 10:\n",
    "\t\t\t\t\tlang2 = assess_language(text2)\n",
    "\n",
    "\t\t\t\tsave_songs(title, text1, text2, lang1, lang2, letter)\n",
    "\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(e)\n",
    "\t\t\n",
    "\tcnt += 1\n",
    "\n",
    "\tprint(f\"Letter {letter}: procesed {cnt}/{artist_cnt}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
